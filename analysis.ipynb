{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INTEG 475 Final Project: Biasedness of Useful Votes in Yelp Reviews\n",
    "\n",
    "|Name        | Student ID|\n",
    "|------------|-----------|\n",
    "|Arumoy Shome| 20460621  |\n",
    "|Shruti Rao  | 20455421  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "From its humble beginings as an email based referral network for doctors, Yelp has evolved into a billion dollar  publisher of crowd-sourced reviews about local businesses. Within Yelp, the value of a business depends directly on the information provided by users in the form of written reviews and photos. Potential customers can then view these user reviews and make more informed decisions about using a particular service provided by a business.\n",
    "\n",
    "To make user reviews more reliable for others, Yelp introduced a system of rating these user reviews as well. A user can manually upvote reviews as `useful` to indicate to other users that this piece of text provided meaningful information about a business from their perspective. However, relying on users to manually indicate utility can introduce bias that is unrelated to the actual relevance of the review text. For local businesses and users who are increasingly reliant on such systems over traditional food critiques, the legitimacy of a review can make or break an experience. \n",
    "\n",
    "The idea of usefulness is a complicated one, affected by user’s perception and needs. In the case of online reviews, the utility of the text to a user is how well it provides relevant information about the interaction with the business in question. However, because different users have different needs, the value of the supposed relevant information within a review will differ. For a system like Yelp, while user labels of value can provide an additional layer of filtering, it is clear that differences exist in exactly how “usefulness” can be defined between individuals. \n",
    "\n",
    "In this project, first a benchmark model is created to predicts a review’s “usefulness” based on review-text. Other models that incorporate other non-review-text related features are then created and compared against the benchmark in order to identify if bias exists when users manually label reviews as “useful”. If bias does exist, knowing the specific features that cause this can be used to better tweak Yelp’s ecosystem to surface unbiased information for users.\n",
    "    \n",
    "    \n",
    "## Research Question\n",
    "Do users actually read a review before voting it useful or is their influenced by the presence of previsouly registered `cool`, `funny` and `useful` votes.\n",
    "\n",
    "\n",
    "## Background\n",
    "There exists a large amount of existing research about online reviews based on the Yelp dataset. The dataset itself is open source and updated with new data annually, encouraging new developments in the review analysis space each year [1]. \n",
    "\n",
    "A literature review found that while there are a variety of different techniques [2, 3] being used to analyze review data, random forest classifiers are incredibly popular [4, 5]. One group used Bag-of-Words on review data  combined with features from user and business datasets to predict the required numeric value of the “usefulness” of a review with a batch mode localized weighted regression model. This localized regression approach resulted into RMSLE of 0.47769 [2]. Others compared SVM with logistic regression with Lasso to identify that the latter was the best with lower training error [3]. However, in comparison, using a random forest model with 150 estimators, the square root of original feature numbers to get maximum features, with ten minimum number of leaves to split, and no maximum depth lead to a model accuracy of 0.698926 [4]. Further improvements using random forest prediction with five fold cross validation were able to achieve an accuracy rate of 79% [5].  Another group took this even further and was able to achieve 95% accuracy in correctly labelling a review as useful using this random forest classifier approach [6]. \n",
    "\n",
    "In parallel with this research into predicting the “usefulness” of a review is developing an understanding of what skews bias in online reviews. Studies use Amazon and Yelp data to look at what features specific to these two ecosystems cause bias in user perception of review value. Typically, a review is seen relative to other reviews. In a study using over 4 million Amazon book reviews for roughly 675 000 books, the perceived usefulness of reviews was dependent on its relationship to other reviews, not only its content [17]. Information about the reviewer themselves can also affect the perceived utility of a review [8]. Yelp restaurant reviews had their “usefulness” affected by user-based features more than detailed information within the review about the features of the restaurant [9]. More specifically, another group looking at over 72 000 Yelp reviews found that neutral and negative reviews by locals were seen as more useful, even if the information within the review was not necessarily valuable [10].\n",
    "\n",
    "The novelty of this paper is found in the granular approach to which Yelp dataset-specific features are considered. Instead of a broad generalization of what creates bias, the focus is on features with high correlation with review “usefulness” in order to understand if they bias users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "from scipy.sparse import csr_matrix, csc_matrix, hstack, vstack\n",
    "SAMPLE_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('dataset/review.csv', usecols=['text', 'useful', 'cool'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that we have equal number of data points for each label. We will label a review `useful` if it has received 1 or more `useful` votes, and those with votes lesser than 1 as *not useful*.\n",
    "\n",
    "The following cell randomly samples for a predefined number (defined by `SAMPLE_SIZE`) of *useful* and *not useful* reviews. The dataframes are then concatenated together and the resulting df is then shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useful_reviews shape:  (5000, 3)\n",
      "not_useful_reviews shape:  (5000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4340862</th>\n",
       "      <td>I can't begin to explain what an amazing place...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3708364</th>\n",
       "      <td>I found Jen at Essentials on Yelp after one so...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827027</th>\n",
       "      <td>Came in here for for a quick beer. Bartenders ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636168</th>\n",
       "      <td>Absolutely amazing selection and variety of pl...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839785</th>\n",
       "      <td>Use to love this place , hadn't been here in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  useful  cool\n",
       "4340862  I can't begin to explain what an amazing place...       0     0\n",
       "3708364  I found Jen at Essentials on Yelp after one so...       0     0\n",
       "1827027  Came in here for for a quick beer. Bartenders ...       0     0\n",
       "1636168  Absolutely amazing selection and variety of pl...       2     2\n",
       "3839785  Use to love this place , hadn't been here in a...       0     0"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "THRESHOLD = 1 \n",
    "\n",
    "useful_reviews = reviews.loc[reviews['useful'] >= THRESHOLD].sample(SAMPLE_SIZE)\n",
    "not_useful_reviews = reviews.loc[reviews['useful'] < THRESHOLD].sample(SAMPLE_SIZE)\n",
    "print(\"useful_reviews shape: \", useful_reviews.shape)\n",
    "print(\"not_useful_reviews shape: \", not_useful_reviews.shape)\n",
    "\n",
    "reviews = shuffle(pd.concat([useful_reviews, not_useful_reviews]))\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Extraction\n",
    "\n",
    "We are using the [`TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for feature extraction from the `text` column. For this column, a custom `analyzer` method namely `review_process` is written which:\n",
    "1. removes all punctuations and\n",
    "3. removes new line characters (escape sequence)\n",
    "\n",
    "`TfidfVectorizer` class is then set to:\n",
    "2. remove accents\n",
    "2. remove all stopwords\n",
    "3. lowercase all words\n",
    "\n",
    "For `useful` votes:\n",
    "2. remove df row if `useful` is `NaN`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews shape after dropping NaN values:  (10000, 3)\n"
     ]
    }
   ],
   "source": [
    "reviews.dropna(inplace=True)\n",
    "print(\"reviews shape after dropping NaN values: \", reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "RE_NEWLINE = '\\n+'\n",
    "PUNCTUATIONS = string.punctuation\n",
    "\n",
    "def review_process(review):\n",
    "    no_newline = re.sub(RE_NEWLINE, '', review)\n",
    "    no_punc = ''.join([char for char in no_newline if char not in PUNCTUATIONS])\n",
    "    \n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a str with some 2 punctuations 6546721 and numbers   \n"
     ]
    }
   ],
   "source": [
    "test = \"a str.. with! some @2 punctuations 6546721 and numbers \\n [] \\n\"\n",
    "print(review_process(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2',\n",
       "        preprocessor=<function review_process at 0x183a10a60>,\n",
       "        smooth_idf=True, stop_words='english', strip_accents='ascii',\n",
       "        sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = TfidfVectorizer(strip_accents='ascii', preprocessor=review_process, stop_words='english' )\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating labels\n",
    "\n",
    "We assign a label of **1** to reviews that are *useful* and a label of **0** to those which are *not useful*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "NOT_USEFUL = 0\n",
    "USEFUL = 1\n",
    "\n",
    "def labeler(vote):\n",
    "    if math.floor(vote) < THRESHOLD:\n",
    "        return NOT_USEFUL\n",
    "    else:\n",
    "        return USEFUL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>cool</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4340862</th>\n",
       "      <td>I can't begin to explain what an amazing place...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3708364</th>\n",
       "      <td>I found Jen at Essentials on Yelp after one so...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827027</th>\n",
       "      <td>Came in here for for a quick beer. Bartenders ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636168</th>\n",
       "      <td>Absolutely amazing selection and variety of pl...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3839785</th>\n",
       "      <td>Use to love this place , hadn't been here in a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  useful  cool  \\\n",
       "4340862  I can't begin to explain what an amazing place...       0     0   \n",
       "3708364  I found Jen at Essentials on Yelp after one so...       0     0   \n",
       "1827027  Came in here for for a quick beer. Bartenders ...       0     0   \n",
       "1636168  Absolutely amazing selection and variety of pl...       2     2   \n",
       "3839785  Use to love this place , hadn't been here in a...       0     0   \n",
       "\n",
       "         label  \n",
       "4340862      0  \n",
       "3708364      0  \n",
       "1827027      0  \n",
       "1636168      1  \n",
       "3839785      0  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['label'] = reviews['useful'].apply(labeler)\n",
    "    \n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "Cross-validation is an approach that prevents over-fitting and helps generalize our model better.\n",
    "We will use [K-fold validation](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold) with 5 folds using `cross_val_score` from `sklearn.model_selection` which by default used K-fold validation. `cross_val_score` trains and validates the classifier with *k-1* folds and tests with the remaining fold which means we do not have to create testing and training splits manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reviews[['text', 'useful', 'cool']]\n",
    "Y = reviews['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_percent(flt):\n",
    "    pretty = \"%.2f\" % (flt*100)\n",
    "    \n",
    "    return \"{0}%\".format(pretty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate classifiers\n",
    "\n",
    "We will be using 3 classifiers for this analysis namely, [*Multinomial Naive Bayes*](http://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes), [*Linear SVM*](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) and [*Random Forest*](http://scikit-learn.org/stable/modules/ensemble.html#random-forests). These classifiers were choosen as they were the most frequent choices in several papers written on the matter [4, 5, 6, 17]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "\n",
      "\n",
      "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "mn_clas = MultinomialNB()\n",
    "print(mn_clas)\n",
    "print(\"\\n\")\n",
    "\n",
    "svc_clas = LinearSVC()\n",
    "print(svc_clas)\n",
    "print(\"\\n\")\n",
    "\n",
    "rf_clas = RandomForestClassifier()\n",
    "print(rf_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Training with `text`\n",
    "\n",
    "Under this model, only the `text` column is used as the training data and the mean of their scores are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X1 after feature extraction:  (10000, 49085)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction\n",
    "X1 = vect.fit_transform(X['text'])\n",
    "print(\"shape of X1 after feature extraction: \", X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes score:  57.45%\n",
      "Linear SVC score:  56.80%\n",
      "Random Forest score:  57.79%\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes score: \", print_percent(mn_valid_scores.mean()))\n",
    "print(\"Linear SVC score: \", print_percent(svc_valid_scores.mean()))\n",
    "print(\"Random Forest score: \", print_percent(rf_valid_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Training with `text` and `useful` Votes\n",
    "\n",
    "Since X now contains mixed dtypes, we need to be a bit clever. First, we convert `useful` column for train and test into sparse matrix and *I2* normalize them (I2 is the default for `TfidfVectorizer` so that's what we use here as well).\n",
    "\n",
    "The rest is straight forward, we create our updated X by stacking the `useful` sparse matrices with X horizontally (such that the number of feature increases). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of useful_sparse_train:  <class 'scipy.sparse.csr.csr_matrix'>\n",
      "shape of useful_sparse_train:  (1, 10000)\n"
     ]
    }
   ],
   "source": [
    "useful_sparse_train = normalize(csr_matrix(X['useful']))\n",
    "print(\"type of useful_sparse_train: \", type(useful_sparse_train))\n",
    "print(\"shape of useful_sparse_train: \", useful_sparse_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hstack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2628b384b338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museful_sparse_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"shape of X2: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hstack' is not defined"
     ]
    }
   ],
   "source": [
    "X2 = hstack([X1, useful_sparse_train.T])\n",
    "print(\"shape of X2: \", X2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the features of X2 have increased from _49085_ to _49086_ since we want to use the `useful` column as a feature for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_scores = cross_val_score(mn_clas, X2, Y, cv=5)\n",
    "svc_scores = cross_val_score(svc_clas, X2, Y, cv=5)\n",
    "rf_scores = cross_val_score(rf_clas, X2, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes score:  58.07%\n",
      "Linear SVC score:  63.15%\n",
      "Random Forest score:  84.49%\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes score: \", print_percent(mn_scores.mean()))\n",
    "print(\"Linear SVC score: \", print_percent(svc_scores.mean()))\n",
    "print(\"Random Forest score: \", print_percent(rf_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Training with `text` and `cool` Votes\n",
    "\n",
    "Next we will carry out the analysis with `text` and `cool` votes. We choose `cool` votes since it had the second highest correlation with `useful` (see section: Data Exploration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cool_sparse_train = normalize(csr_matrix(X['cool']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X3:  (10000, 48400)\n"
     ]
    }
   ],
   "source": [
    "X3 = hstack([X1, cool_sparse_train.T])\n",
    "print(\"shape of X3: \", X3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_scores = cross_val_score(mn_clas, X3, Y, cv=5)\n",
    "svc_scores = cross_val_score(svc_clas, X3, Y, cv=5)\n",
    "rf_scores = cross_val_score(rf_clas, X3, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes score:  57.66%\n",
      "Linear SVC score:  59.56%\n",
      "Random Forest score:  65.06%\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes score: \", print_percent(mn_scores.mean()))\n",
    "print(\"Linear SVC score: \", print_percent(svc_scores.mean()))\n",
    "print(\"Random Forest score: \", print_percent(rf_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Training with `text`, `useful` and `cool` Votes\n",
    "\n",
    "Finally we will carry out the analysis with text, `useful` & `cool` votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of uc_sparse_train:  (10000, 2)\n"
     ]
    }
   ],
   "source": [
    "uc_sparse_train = normalize(csr_matrix(X[['cool', 'useful']]))\n",
    "print(\"shape of uc_sparse_train: \", uc_sparse_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X4:  (10000, 49087)\n"
     ]
    }
   ],
   "source": [
    "X4 = hstack([X1, uc_sparse_train])\n",
    "print(\"shape of X4: \", X4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the features of X4 have increased from _49085_ to _49087_ since now we have the `useful` and `cool` columns as features for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn_scores = cross_val_score(mn_clas, X4, Y, cv=5)\n",
    "svc_scores = cross_val_score(svc_clas, X4, Y, cv=5)\n",
    "rf_scores = cross_val_score(rf_clas, X4, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes score:  82.10%\n",
      "Linear SVC score:  99.99%\n",
      "Random Forest score:  86.41%\n"
     ]
    }
   ],
   "source": [
    "print(\"Naive Bayes score: \", print_percent(mn_scores.mean()))\n",
    "print(\"Linear SVC score: \", print_percent(svc_scores.mean()))\n",
    "print(\"Random Forest score: \", print_percent(rf_scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The goal of this project was to determine whether `useful` votes received by reviews on Yelp are biased. To understand the presence of any such relationship, we carried out training in four stages:\n",
    "1. To establish a benchmark, we carried out supervised learning of our model on the text of reviews.\n",
    "2. Next, the useful votes previously received by reviews are also added during training.\n",
    "3. For the third model, we trained with review text and `cool` votes which has the highest correlation with `useful` votes.\n",
    "4. Finally, for the fourth model we trained with text, `useful` and `cool` votes.\n",
    "\n",
    "**We observed that reviews that obtained a previously high number of `useful` votes were more likely to be voted `useful` in the future.**\n",
    "Model 1 and Model 3 share similar scores implying that `cool` votes dont cause too much bias towards usefulness of a review.\n",
    "Finally, Model 4 had the highest scores meaning `useful` and `cool` votes together cause maximum bias.\n",
    "\n",
    "\n",
    "| Models | Multinomial Naive Bayes| Linear SVM| Random Forest|\n",
    "|--------|------------------------|-----------|--------------|\n",
    "| Model 1|57.45%                  |56.80%     |57.79%        |\n",
    "| Model 2|58.07%                  |63.15%     |84.49%        |\n",
    "| Model 3|57.66%                  |59.56%     |65.06%        |\n",
    "| Model 4|82.10%                  |99.99%     |86.41%        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendations\n",
    "\n",
    "1. After feature extraction, [feature selection](http://scikit-learn.org/stable/modules/feature_selection.html) will be helpful once the number of data points increases. Feature selection is a process whereby certain features within the model with low variance are removed. This is useful for us for the following reasons:\n",
    "    * reduces the training time of our classifiers\n",
    "    * reduces overfitting by generalizing the model\n",
    "    * reduces the complexity of the model as there are less features to deal with\n",
    "\n",
    "2. Improve label generation by using k-means to identify the labels\n",
    "Presently a Naive methodology has been used to determine the labels for the *useful* votes. A better solution would be to use [k-means](http://scikit-learn.org/stable/modules/clustering.html#k-means) to determine appropriate labels on the *useful* votes through clustering. This is because currently we are assuming that there are only two labels - *useful* and *not useful*; however, the data might indicate the presence of more than two such as *not useful*, *slightly useful*, *useful*, *very useful*, etc.\n",
    "\n",
    "3. Next, due to hardware limitations, we were only able to work with 10,000 data points. However, based on the [*learning curves*](http://scikit-learn.org/stable/modules/learning_curve.html#learning-curve) generated below for model 4 classifiers, we see that our models prediction rate will improve with increase in data points. \n",
    "\n",
    "4. Finally, additional [non-text features](https://www.yelp.ca/dataset/documentation/json) can be included to establish a stronger case for biasedness and pinpoint the specific features causing bias. Examples of such features are:\n",
    "    * presence of photos in reviews\n",
    "    * `useful` and `cool` votes given to users themselves for their contributions\n",
    "    * number of `fans` that the users have\n",
    "    * attributes of businesses such as `stars` rating and `number of reviews` they recieved\n",
    "\n",
    "Futhermore, bias due to ethicicty, demography, location, etc. of reviewers should be considered in further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_curve' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-61a31095fb82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmn_train_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmn_train_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmn_valid_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn_clas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msvc_train_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_train_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc_valid_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvc_clas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mrf_train_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_train_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_valid_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_clas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_curve' is not defined"
     ]
    }
   ],
   "source": [
    "mn_train_sizes, mn_train_scores, mn_valid_scores = learning_curve(mn_clas, X4, Y, cv=5)\n",
    "svc_train_sizes, svc_train_scores, svc_valid_scores = learning_curve(svc_clas, X4, Y, cv=5)\n",
    "rf_train_sizes, rf_train_scores, rf_valid_scores = learning_curve(rf_clas, X4, Y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_curve_data = {'Multinomial Bayes': [mn_train_sizes, mn_train_scores, mn_valid_scores],\n",
    "                       'Linear SVM': [svc_train_sizes, svc_train_scores, svc_valid_scores],\n",
    "                       'Random Forest': [rf_train_sizes, rf_train_scores, rf_valid_scores]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'learning_curve_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7db443e3770a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlearning_curve_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Learning Curves  - {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'learning_curve_data' is not defined"
     ]
    }
   ],
   "source": [
    "## source: http://scikit-learn.org/stable/auto_examples/model_selection\n",
    "## /plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py \n",
    "\n",
    "for name, data in learning_curve_data.items():\n",
    "    plt.figure()\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.title(\"Learning Curves  - {0}\".format(name))\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Scores\")\n",
    "\n",
    "    train_scores_mean = np.mean(data[1], axis=1)\n",
    "    train_scores_std = np.std(data[1], axis=1)\n",
    "\n",
    "    test_scores_mean = np.mean(data[2], axis=1)\n",
    "    test_scores_std = np.std(data[2], axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(data[0], train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(data[0], test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                     color=\"g\")\n",
    "    plt.plot(data[0], train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(data[0], test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "[1] \"Yelp Dataset\", Yelp, 2018. [Online]. Available: https://www.yelp.ca/dataset/. [Accessed: 09- Apr- 2018].\n",
    "\n",
    "[2] R. Shen, J. Shen, Y. Li and H. Wang, \"Predicting usefulness of Yelp reviews with localized linear regression models\", 2016 7th IEEE International Conference on Software Engineering and Service Science (ICSESS), 2016.\n",
    "\n",
    "[3] X. Liu, M. Schoemaker and N. Zhang, \"Predicting Usefulness of Yelp Reviews\", Stanford University, Stanford, 2014. http://cs229.stanford.edu/proj2014/Xinyue%20Liu,%20Michel%20Schoemaker,%20Nan%20Zhang,Predicting%20Usefulness%20of%20Yelp%20Reviews.pdf\n",
    "\n",
    "[4] H. Zhang, X. Li and K. Ying, \"Reviews Usefulness Prediction for Yelp Dataset\", UC San Diego, San Diego, 2014. https://cseweb.ucsd.edu/classes/wi17/cse258-a/reports/a040.pdf\n",
    "\n",
    "[5] M. Barakat, \"Predicting the Usefulness of a Yelp Review Using Machine Learning\", 2018. [Online]. Available: https://rstudio-pubs-static.s3.amazonaws.com/133133_fd7291a46ff547288a25af57b6b35100.html. [Accessed: 07- Apr- 2018].\n",
    "\n",
    "[6] A. Ghenai, \"What makes a review useful, funny or cool on Yelp.com\", Project report for CS886: Applied Machine Learning, 2015.\n",
    "\n",
    "[7] C. Danescu-Niculescu-Mizil, G. Kossinets, J. Kleinberg and L. Lee, \"How opinions are received by online communities\", Proceedings of the 18th international conference on World wide web - WWW '09, 2009.\n",
    "\n",
    "[8] P. Racherla and W. Friske, \"Perceived ‘usefulness’ of online consumer reviews: An exploratory investigation across three services categories\", Electronic Commerce Research and Applications, vol. 11, no. 6, pp. 548-559, 2012.\n",
    "\n",
    "[9] L. Li, K. Zhang, Q. Zhou and C. Zhang, \"Toward Understanding Review Usefulness: A Case Study on Yelp Restaurants\", iConference 2016 Proceedings. 2016.\n",
    "\n",
    "[10] J. Neumann, D. Gutt, D. Kundisch, and D. van Straaten, “When Local Praise Becomes Cheap Talk - Analyzing the Relationship between Reviewer Location and Usefulness of Online Reviews.” Proceedings of the Multikonferenz Wirtschaftsinformatik 2018 (MKWI), Lüneburg, Germany, 2018"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
